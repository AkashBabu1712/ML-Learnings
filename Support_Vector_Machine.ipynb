{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJMhz3/muz1Ngg8w2TxyWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkashBabu1712/Machine-Learning-/blob/main/Support_Vector_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yb8GgF8IRVmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://datatron.com/wp-content/uploads/2021/05/Support-Vector-Machine.png)"
      ],
      "metadata": {
        "id": "qjBIMhUGSRz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A support vector machine (SVM) is a supervised machine learning model that uses classification techniques for solving two-group classification problems.\n",
        "\n",
        "* SVM models can categorize new text after being given labeled training data sets for each category.\n",
        "\n",
        "* These models offer two key advantages over newer algorithms like neural networks: faster processing and better performance with fewer samples (in the thousands).\n",
        "\n"
      ],
      "metadata": {
        "id": "iQveLoE1RdHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Terminologies of SVM"
      ],
      "metadata": {
        "id": "r-9DE33dTBBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Hyperplane:**\n",
        "\n",
        "* Hyperplanes, also known as decision boundaries or decision planes, are the boundaries that aid in the classification of data points. The side of the hyperplane where a new data point lands can be divided into multiple groups\n",
        "\n",
        "* The hyperplane’s dimension is determined by the number of features assigned to a dataset. The hyperplane can be a simple line if the dataset only has two features.\n",
        "\n",
        "* A hyperplane is a two-dimensional plane when a dataset comprises three features.\n",
        "\n",
        "\n",
        "2. **Support Vectors**:\n",
        "\n",
        "  Support vectors are the data points closest to the hyperplane and affect its position. These vectors are called support vectors because they alter hyperplane location, hence the Support Vector Machine Algorithm.\n",
        "\n",
        "3. **Margin:**\n",
        "\n",
        "* The distance between the hyperplane and the support vectors is the margin.\n",
        "\n",
        "* The hyperplane that optimizes the margin is always chosen using SVM. The wider the margin, the more accurate the results are.\n",
        "\n",
        "* There are two types of margins in SVM algorithms: hard and soft."
      ],
      "metadata": {
        "id": "V7ijcsV5TC6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of SVM"
      ],
      "metadata": {
        "id": "tXm_apnWUf-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Linear SVM:** For a linearly separable dataset, linear SVM is employed.\n",
        "\n",
        "***\n",
        "\n",
        " ![](https://www.shiksha.com/online-courses/articles/wp-content/uploads/sites/11/2022/05/image-100-386x330.png)\n",
        "\n",
        "\n",
        "2. **Non-linear SVM:**\n",
        "\n",
        "* Separating the dataset linearly becomes difficult as the number of features grows. A non-linear SVM is used in this case. When the dataset is not linearly separable, we can’t draw a straight line to separate data points.\n",
        "\n",
        "  **Z = x2 + Y2**\n",
        "\n",
        "* When a data point is turned into a high-dimensional space by adding a new dimension, a hyperplane can readily separate it. This is accomplished via a technique known as the kernel trick. SVM algorithms can convert non-separable data into separable data using the kernel method.\n",
        "\n",
        "***\n",
        "\n",
        "  ![](https://www.shiksha.com/online-courses/articles/wp-content/uploads/sites/11/2022/05/non-linear-440x257.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PDCpzahpUnDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation with code on Dataset"
      ],
      "metadata": {
        "id": "TS7dtjpXW6FP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some important hyperparameters that should be considered before training the model:**\n",
        "\n",
        "**C:** The regularization parameter has a float value and is 1.0 by default. Must be strictly positive.\n",
        "\n",
        "**kernel:** Specifies the kernel type used in the algorithm. ‘linear,’ ‘poly,’ ‘rbf.’\n",
        "\n",
        "**degree:** An optional integer value to specify the degree of polynomial kernel function which is ignored by all other kernels\n",
        "\n",
        "**gamma:** Kernel coefficient for ‘rbf’,’poly’\n",
        "\n",
        "**coef0:** Independent term in kernel function significant in ‘poly.’"
      ],
      "metadata": {
        "id": "NOlmvb5yc0PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import scikit-learn dataset library\n",
        "from sklearn import datasets\n",
        "\n",
        "#Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n"
      ],
      "metadata": {
        "id": "Js3B1a-relxm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Data\n",
        "\n",
        "# print the names of the 13 features\n",
        "print(\"Features: \", cancer.feature_names)\n",
        "\n",
        "# print the label type of cancer('malignant' 'benign')\n",
        "print(\"Labels: \", cancer.target_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM7UwLUXgN77",
        "outputId": "378db292-c291-469d-8e1e-9e25ef1ab91f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
            "Labels:  ['malignant' 'benign']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print data(feature)shape\n",
        "cancer.data.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm2kErxsgYgl",
        "outputId": "f0360536-4059-4635-d037-1afaee328d3b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the cancer data features (top 5 records)\n",
        "print(cancer.data[0:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIynryNpgd6U",
        "outputId": "7c6ac7ae-2d3b-4b1d-b337-09a369425d3d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
            "  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
            "  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
            "  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
            "  4.601e-01 1.189e-01]\n",
            " [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
            "  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
            "  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
            "  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
            "  2.750e-01 8.902e-02]\n",
            " [1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n",
            "  1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n",
            "  6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n",
            "  2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n",
            "  3.613e-01 8.758e-02]\n",
            " [1.142e+01 2.038e+01 7.758e+01 3.861e+02 1.425e-01 2.839e-01 2.414e-01\n",
            "  1.052e-01 2.597e-01 9.744e-02 4.956e-01 1.156e+00 3.445e+00 2.723e+01\n",
            "  9.110e-03 7.458e-02 5.661e-02 1.867e-02 5.963e-02 9.208e-03 1.491e+01\n",
            "  2.650e+01 9.887e+01 5.677e+02 2.098e-01 8.663e-01 6.869e-01 2.575e-01\n",
            "  6.638e-01 1.730e-01]\n",
            " [2.029e+01 1.434e+01 1.351e+02 1.297e+03 1.003e-01 1.328e-01 1.980e-01\n",
            "  1.043e-01 1.809e-01 5.883e-02 7.572e-01 7.813e-01 5.438e+00 9.444e+01\n",
            "  1.149e-02 2.461e-02 5.688e-02 1.885e-02 1.756e-02 5.115e-03 2.254e+01\n",
            "  1.667e+01 1.522e+02 1.575e+03 1.374e-01 2.050e-01 4.000e-01 1.625e-01\n",
            "  2.364e-01 7.678e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the cancer labels (0:malignant, 1:benign)\n",
        "print(cancer.target)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0HmlnqOggze",
        "outputId": "172bc7e1-2cce-4754-d376-3be1e8ea4782"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting Data\n",
        "\n",
        "# Import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109) # 70% training and 30% test\n"
      ],
      "metadata": {
        "id": "xgQcibrDglf1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating Model\n",
        "\n",
        "#Import svm model\n",
        "from sklearn import svm\n",
        "\n",
        "#Create a svm Classifier\n",
        "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
        "\n",
        "#Train the model using the training sets\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "D0lUyYaZgrfO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the Model\n",
        "\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6crBws7gxmm",
        "outputId": "c3c85586-9e7c-4120-9c36-5c5e050905ca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**got a classification rate of 96.49%, considered as very good accuracy.**"
      ],
      "metadata": {
        "id": "Md7TAYqbg_I0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCKdWkU8g4-u",
        "outputId": "cf29ff66-d230-4a49-c106-dd249ad0584b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9811320754716981\n",
            "Recall: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**got a precision of 98% and recall of 96%, which are considered as very good values**"
      ],
      "metadata": {
        "id": "pi07rKfWhMiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning Hyperparameters\n",
        "\n"
      ],
      "metadata": {
        "id": "XpF5Iro8hUML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kernel:**\n",
        "\n",
        "* The main function of the kernel is to transform the given dataset input data into the required form. There are various types of functions such as linear, polynomial, and radial basis function (RBF).\n",
        "\n",
        "* Polynomial and RBF are useful for non-linear hyperplane. Polynomial and RBF kernels compute the separation line in the higher dimension. In some of the applications, it is suggested to use a more complex kernel to separate the classes that are curved or nonlinear. This transformation can lead to more accurate classifiers.\n",
        "\n",
        "**Regularization:**\n",
        "\n",
        "* Regularization parameter in python's Scikit-learn C parameter used to maintain regularization. Here C is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimization how much error is bearable.\n",
        "\n",
        "* This is how you can control the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.\n",
        "\n",
        "\n",
        "**Gamma:**\n",
        "\n",
        "* A lower value of Gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting.\n",
        "\n",
        "* In other words, you can say a low value of gamma considers only nearby points in calculating the separation line, while the a value of gamma considers all the data points in the calculation of the separation line."
      ],
      "metadata": {
        "id": "zK57TJWUh_F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages**\n",
        "\n",
        "SVM Classifiers offer good accuracy and perform faster prediction compared to Naïve Bayes algorithm. They also use less memory because they use a subset of training points in the decision phase. SVM works well with a clear margin of separation and with high dimensional space.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "SVM is not suitable for large datasets because of its high training time and it also takes more time in training compared to Naïve Bayes. It works poorly with overlapping classes and is also sensitive to the type of kernel used."
      ],
      "metadata": {
        "id": "mOTHicNjiPAV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gCDE6aJehH3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}